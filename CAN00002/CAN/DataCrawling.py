# -*- coding = utf-8 -*-# @Time : 2020/12/7 10:44# @Author : Claire Shiyu Chen# @Author : Terence Tianyang Wang# @Author : Milo Dongfeng Li# @Software : Pycharmfrom lxml import etreeimport numpy as npimport reimport urllib.request, urllib.errorimport sslimport urllib.parsessl._create_default_https_context = ssl._create_unverified_context# After entering the keyword, the key information on the page is crawled and weighted# Return the following items in the end:# The information of the top 5 articles, the most citation of the articles,# the total number of the research results, the average year, the average cite,# the total number of the citation(Divide by 9000 for curve), the related researchingdef main(keyword):    '''    :param keyword: keyword to search    :return: a dict like {"show5": show5, "citefirst": citefirst, "totalNum": totalNum, 'yearAvg': yearAvg, 'citeAvg': citeAvg,            'averagecite': averagecite, 'relateds': relateds}    '''    papers = []    relatedTitles = []    relatedRefs = []    for startPage in range(0, 100, 10):        baseurl = 'https://scholar.google.com.hk/scholar?start=' + str(startPage) + '&hl=en&q=' + keyword + '&btnG=&lr='        content = askURL(baseurl)        dom = etree.HTML(content)        nodes = dom.xpath('//div[@class="gs_r gs_or gs_scl"]')        for node in nodes:            title = node.xpath('string(./div/h3/a)')            title = " ".join(title.split())            titleRef = node.xpath('.//div/h3/a/@href')            year = node.xpath('string(.//div[@class="gs_a"])')            cite = node.xpath('string(.//a[contains(@href,"?cites=")])')            citeRefNodes = node.xpath('.//a[contains(@href,"?cites=")]/@href')            citeRef = ''            if len(citeRefNodes) > 0:                citeRef = 'https://scholar.google.com' + citeRefNodes[0]            version = node.xpath('string(.//a[@class="gs_nph"])')            pdfRef = node.xpath('string(.//div[@class="gs_or_ggsm"]/a/@href)')            paper = {                'title': getListstr(title),                'titleRef': getListstr(titleRef),                'year': yearFilter(year),                'cite': digitFilter(cite),                'citeRef': getListstr(citeRef),                'version': digitFilter(version),                'pdfRef': getListstr(pdfRef),            }            if paperValidator(paper):                papers.append(paper)        relatedNodes = dom.xpath('//div[@class="gs_qsuggest gs_qsuggest_regular"]//ul/li/a')        relatedTitles.extend(getNodesStr(relatedNodes))        relatedRefs.extend(getNodesHref(relatedNodes))    totalNum = digitFilter(dom.xpath('string(.//*[@id="gs_ab_md"]/div)'))    relateds = list(zip(relatedTitles, relatedRefs))    papers.sort(key=sortCmp, reverse=True)    show5 = [papers[0], papers[1], papers[2], papers[3], papers[4]]    yearsall = normalize(papers, 'year')    citesall = normalize(papers, 'cite')    yearAvg = str(avgPapers(yearsall))    citeAvg = str(avgPapers(citesall))    cited = np.asarray(normalize(papers, 'cite'))    citefirst = np.max(cited)    averagecite = avgCite(papers) / 9000    years5 = normalize(show5, 'year')    for i in range(5):        pic = {            'year5': years5[i],            'yscore': yearsall[i],            'versions': show5[i].get('version'),            'cite5': citesall[i],            'cscore': citesall[i],        }        show5[i]['pic'] = pic    return {"show5": show5, "citefirst": citefirst, "totalNum": totalNum, 'yearAvg': yearAvg, 'citeAvg': citeAvg,            'averagecite': averagecite, 'relateds': relateds}# Judge whether the information in the article is complete or not, and discard it if it is incompletedef paperValidator(paper):    if paper.get('title').strip()=='':        return 0    if paper.get('titleRef').strip()=='':        return 0    if paper.get('citeRef').strip()=='':        return 0    return 1# Output the text as a Stringdef getListstr(list):    if (type(list).__name__ == 'list'):        if len(list)==0:            return ''        list.sort(key=len, reverse=True)        return list[0]    return list# Normalize the input numbers array and output between 0 and 5def normalize(papers, keys):    array = []    for paper in papers:        array.append(int(paper.get(keys)))    list = []    for x in array:        x = float(x - min(array)) / (max(array) - min(array) + 1)        list.append(x * 5)    return list# The number of year and cite in the article is weighted by 7:3def sortCmp(paper):    year = paper.get('year')    cite = paper.get('cite')    if year.strip() == '':        year = '0'    if cite.strip() == '':        cite = '0'    return int(year) * 7 + int(cite) * 3# Find the weighted average of the inputdef avgPapers(normalize):    total = 0    for x in normalize:        total += x    return round(total / len(normalize))# Find the average of the citations of the input papersdef avgCite(papers):    total = 0    for paper in papers:        total += int(paper.get('cite'))    return round(total / len(papers))# Get the string of related researchingdef getNodesStr(nodes):    nodesStr = []    for node in nodes:        nodesStr.append(node.xpath('string(.)'))    return nodesStr# Gets the number in the inputdef digitFilter(input):    digit = ''.join(list(filter(str.isdigit, input)))    if digit.strip() == '':        return '0'    return digit# Verify that the year in the input is the actual year the article was published# If not, return 0def yearFilter(input):    pattern = re.compile(r'[1-2]\d\d\d-', re.I)    years = pattern.findall(input.replace(" ", ""))    for year in years:        if int(year.replace("-", "")) < 2021:            return year.replace("-", "")    return '0'# Gets the string of the URL of related researchingdef getNodesHref(nodes):    nodesStr = []    for node in nodes:        if not str(node.xpath('@href')[0]).startswith("http"):            nodesStr.append('https://scholar.google.com.hk' + node.xpath('@href')[0])        else:            nodesStr.append(node.xpath('@href')[0])    return nodesStr# Access to the input webpage and return the source code from the webpagedef askURL(baseurl):    head = {        'Host': 'scholar.google.com',        'Cookie': 'ANID=AHWqTUmI79XFxfG4KsAWg0lpGpggy5USAUV2wQUNTrnHvGBUaV9GDQtq2zwQdx-Q; 1P_JAR=2020-12-10-08; NID=204=Q5JSI73wW3soL4O8B0m_11V3mmfNszL9Umuck1LUJfruXYSyq0PYQ7REk2n4JgMAL8cOVn_NVwS1IfOV9_A-eaEtZVVya8Vv7bZO8RAHFeOs854lvGQF9_ivxecX4es3MyLT_J65i9R7OhxMrbu1dPjOeplzUQKU4ztQb6A9gxup-5rrF3d9mgkjYA; GSP=A=XRH1CA:CPTS=1607588576:LM=1607588576:S=HpjgkYng-l93dKUw',        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36',        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',        'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',        'Referer': 'https://scholar.google.com/schhp?hl=zh-CN',        'Connection': 'keep-alive'    }    html = ""    req = urllib.request.Request(baseurl, headers=head)    try:        response = urllib.request.urlopen(req, timeout=120)        html = response.read()        print("conneect succeed!")    except urllib.error.URLError as e:        if hasattr(e, "code"):            print(e.code)        if hasattr(e, "reason"):            print(e.reason)        else:            print("Time out")    return htmlif __name__ == "__main__":    res=main('physics')    print(res)